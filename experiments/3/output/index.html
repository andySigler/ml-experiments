<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <link type="text/css" rel="stylesheet" href="./css/main.css">
        <script type="text/javascript" src="./main.js"></script>
    </head>
    <body>
        <div class="container">
            <h1>ML-Experiment: My Doodling Style</h1>
            <!-- <div id="generate">
                <h2>Generate Data</h2>
                <button id="saveButton">Save</button>
                <button id="clearButton">Clear</button>
                <div id="canvasParent"></div>
                <h3>Links to CSV Data</h3>
                <div id="fileLinks"></div>
            </div> -->
            <div>
                <span id="decodingMessage"></span>
            </div>
            <div>
                <h2>There Are No Failures</h2>
                <p>
                    Below is a Tensorflow project I worked on for a couple weeks. The final output is not impressive (I think)... however I did a few things for my very first time which I think are worth documenting:
                </p>
                <ul>
                    <li>Used a mixture density network (MDN) to recreate "real-world" samples</li>
                    <li><a href="https://github.com/andySigler/keras-mdn-layer">Forked an open-source MDN Tensorflow implementation</a> to 1) work in TF2.0, and 2) to sample from it in the browser with TFJS</li>
                    <li>Created a larger trained model, by first sub-training smaller parts, then combining them later</li>
                </ul>
                <p>
                    The <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/3/src">source code for this webpage can be found here</a>, which includes the inference model, training playback, and MDN sampling functions. In addition, the <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/3/python">Python notebook I used for training can be found here</a>.
                </p>
            </div>
            <div id="mdn" hidden="true">
                <h2>Model Playback</h2>
                <ol>
                    <li>Press the "Predict" model, and it will start drawing</li>
                    <li>Once it hits the edge of the canvas, it will stop drawing</li>
                </ol>
                <p>
                    ... looks like a mess, right?
                </p>
                <div>
                    <button id="startMDNButton" disabled="true">Predict</button>
                    <button id="stopMDNButton" disabled="true">Stop</button>
                    <span hidden="true">Temp<input type="text" id="tempInput" value="1.0"></span>
                    <span hidden="true">Sigma-Temp<input type="text" id="sigmaTempInput" value="0.02"></span>
                </div>
                <div id="canvasParent"></div>
            </div>
            <div>
                <h2>The Goal</h2>
                <p>
                    I wanted to make a system that can copy how I doodle in my notebooks. Below are some pictures from my notebooks over the years, showing how I doodle.
                </p>
                <img src="./img/my_doodles.jpg">
                <p>
                    You'll notice that, generally, I draw random lines, and I try not to overlap those lines. There are also some common angles and shapes that I subconsciously make.
                </p>
                <p>
                    I've made small programs in the past that try to achieve this programmatically, however it always looked a little too robotic.
                </p>
            </div>
            <div>
                <h2>Did It Work?</h2>
                <p>
                    Not really, but I can say that after working on this model for a couple weeks, I learned quite a lot about how to make it work better.
                </p>
                <p>
                    I think one major thing that could have make it better was using training drawings that are <strong>NOT</strong> just my random doodles. Random doodles are hard for me (a human) to tell apart from just noise when a ML model is generating them...
                </p>
            </div>
            <div>
                <h2>Getting Some Data</h2>
                <p>
                    To start with, I needed to create a system for recording my doodling style. I created a program that records my drawings in an HTML5 Canvas, and then drew for a couple hours while listening to podcasts.
                </p>
                <p>
                    You can see some of my drawings below. Press the "Random" button to see one of my doodles being drawn.
                </p>
                <div id="playback" hidden="true">
                    <!-- <h2>Train Data Playback</h2> -->
                    <div>
                        <button id="startPlaybackButton" disabled="true">Random</button>
                        <button id="stopPlaybackButton" disabled="true">Stop</button>
                    </div>
                    <div id="canvasParent"></div>
                </div>
            </div>
            <div>
                <h2>Training the Model</h2>
                <p>
                    I used a Python notebook to train the models using Tensorflow 2.0-beta.
                </p>
                <p>
                    The training data (my drawings) were loaded and sent through the following processes:
                    <ol>
                        <li>
                            Compress total points, to keep same shape with less data
                            <ul>
                                <li>This allowed me to keep the final model as small as possible, while still being able to generate recognizable shapes</li>
                            </ul>
                        </li>
                        <li>
                            Synthetically create new drawings (more data!), through:
                            <ul>
                                <li>Rotating each drawing four times at 90 degrees</li>
                                <li>Scaling each drawing down in size and transposing</li>
                            </ul>
                        </li>
                        <li>
                            Creating training X/Y pairs, for both the simple autoencoders, and the RNN models
                        </li>
                    </ol>
                </p>
                <p>
                    The final model is a combination of three sequential models:
                    <ol>
                        <li>Convolutional Encoder</li>
                        <li>LSTM</li>
                        <li>Mixture Density Network</li>
                    </ol>
                </p>
                <p>
                    In the diagram below, I tried to demonstrate the sequence of how I trained each model independently. Training each sub-model separately seemed to help the training process learn faster and more accurately than training end-to-end.
                </p>
                <div>
                    <img src="./img/doodle_model_diagram.jpg">
                </div>
            </div>
            <br><br><br>
        </div>
    </body>
</html>
