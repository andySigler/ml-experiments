{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doodle_mdn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLiFTvhePXac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1 --upgrade\n",
        "!pip install tensorflow-gpu==2.0.0-beta1 --upgrade\n",
        "!pip install tfp-nightly --upgrade\n",
        "\n",
        "!pip install git+git://github.com/andysigler/keras-mdn-layer.git@master#egg=keras-mdn-layer --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL1xZpm33jrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from datetime import datetime\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow Version: ', tf.__version__)\n",
        "print('GPU: ', tf.test.gpu_device_name())\n",
        "from tensorflow.keras import Sequential, layers, models, Input, optimizers\n",
        "\n",
        "import mdn\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive_data_folder = '/content/gdrive/My Drive/Colab Notebooks/data/doodle_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72aVGAhzdJnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_model_folder = os.path.join(drive_data_folder, 'models')\n",
        "\n",
        "def dated_filename():\n",
        "  return str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
        "\n",
        "\n",
        "def save_model(model, name):\n",
        "  blob_name = '{0}_{1}.h5'.format(name, dated_filename())\n",
        "  print('Saving -> {}'.format(blob_name))\n",
        "  model.save(os.path.join(drive_model_folder, blob_name))\n",
        "\n",
        "\n",
        "def load_latest_model(name, custom_objects={}):\n",
        "  model_files = [\n",
        "      f\n",
        "      for f in os.listdir(drive_model_folder)\n",
        "      if name in f and 'h5' in f\n",
        "  ]\n",
        "  model_files.sort()\n",
        "  latest_file_name = model_files[-1]\n",
        "  latest_filepath = os.path.join(drive_model_folder, latest_file_name)\n",
        "  print('Loading -> {}'.format(latest_filepath))\n",
        "  model = tf.keras.models.load_model(\n",
        "      latest_filepath, custom_objects=custom_objects)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t7zqaEYFKBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(path):\n",
        "  file_data = []\n",
        "  with open(path, 'r') as f:\n",
        "    for line in iter(f):\n",
        "      line = line.strip()\n",
        "      sample = line.split(',')\n",
        "      file_data.append({\n",
        "          'x': float(sample[1]),\n",
        "          'y': float(sample[2])\n",
        "      })\n",
        "  return file_data\n",
        "\n",
        "\n",
        "def get_cartesian(x1, y1, x2, y2):\n",
        "  x_diff = x2 - x1\n",
        "  y_diff = y2 - y1\n",
        "  hyp = math.sqrt((x_diff * x_diff) + (y_diff * y_diff))\n",
        "  rad = None\n",
        "  if x_diff == 0:\n",
        "    if y_diff > 0:\n",
        "      rad = math.pi * 0.5\n",
        "    else:\n",
        "      rad = math.pi * 1.5\n",
        "  else:\n",
        "    rad = math.atan(y_diff / x_diff)\n",
        "  return (rad, hyp)\n",
        "\n",
        "\n",
        "def get_abs_radian_diff(rad_1, rad_2):\n",
        "  abs_diff = abs(rad_2 - rad_1)\n",
        "  if abs_diff > math.pi:\n",
        "    abs_diff = (2 * math.pi) - abs_diff\n",
        "  return abs_diff\n",
        "\n",
        "\n",
        "def did_move_too_much(prev_rad, new_rad, hypotenuse):\n",
        "  abs_diff_rad = get_abs_radian_diff(prev_rad, new_rad)\n",
        "  if abs_diff_rad > (math.pi / 8):\n",
        "    return True\n",
        "  if (abs_diff_rad * hypotenuse) > (math.pi / 360.0):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def trim_abs_data(data):\n",
        "  trimmed_data = [data[0]]\n",
        "  prev_rad, _ = get_cartesian(\n",
        "    trimmed_data[-1]['x'], trimmed_data[-1]['y'], data[1]['x'], data[1]['y']\n",
        "  )\n",
        "  for i in range(1, len(data)):\n",
        "    new_rad, hypotenuse = get_cartesian(\n",
        "      trimmed_data[-1]['x'], trimmed_data[-1]['y'], data[i]['x'], data[i]['y']\n",
        "    )\n",
        "    if did_move_too_much(prev_rad, new_rad, hypotenuse):\n",
        "      trimmed_data.append(data[i - 1])\n",
        "      prev_rad, _ = get_cartesian(\n",
        "        trimmed_data[-1]['x'], trimmed_data[-1]['y'], data[i]['x'], data[i]['y']\n",
        "      )\n",
        "  trimmed_data.append(data[-1])\n",
        "  return trimmed_data\n",
        "\n",
        "\n",
        "def invert_abs_data(data):\n",
        "  inverted_data = [\n",
        "    {'x': 1.0 - d['x'], 'y': 1.0 - d['y']}\n",
        "    for d in data\n",
        "  ]\n",
        "  flipped_data = [\n",
        "    {'x': d['y'], 'y': d['x']}\n",
        "    for d in data\n",
        "  ]\n",
        "  flipped_inverted_data = [\n",
        "    {'x': 1.0 - d['x'], 'y': 1.0 - d['y']}\n",
        "    for d in flipped_data\n",
        "  ]\n",
        "  return (data, inverted_data, flipped_data, flipped_inverted_data)\n",
        "\n",
        "\n",
        "def scale_drawing(data, scale_list):\n",
        "  return [\n",
        "    [\n",
        "      {\n",
        "        l: (d[l] * scale) + ((1.0 - scale) / 2)\n",
        "        for l in 'xy'\n",
        "      }\n",
        "      for d in data\n",
        "    ]\n",
        "    for scale in scale_list\n",
        "  ]\n",
        "\n",
        "\n",
        "def abs_data_to_delta_tensor(data):\n",
        "  d_data = [\n",
        "    [\n",
        "      data[i][l] - data[i - 1][l]\n",
        "      for l in 'xy'\n",
        "    ]\n",
        "    for i in range(1, len(data))\n",
        "  ]\n",
        "  # add the ABS XY data to the end of the sample list\n",
        "  for i in range(1, len(data)):\n",
        "    d_data[i - 1].append(data[i]['x'])\n",
        "    d_data[i - 1].append(data[i]['y'])\n",
        "  return tf.constant(\n",
        "    d_data, shape=(len(d_data), len(d_data[0])), dtype='float32')\n",
        "\n",
        "\n",
        "class SeqLenException(Exception):\n",
        "  pass\n",
        "\n",
        "\n",
        "def samples_to_sequences(t, seq_len):\n",
        "  if t.shape[0] < seq_len + 1:  # one sample for the final Y value\n",
        "    raise SeqLenException(\n",
        "        'Length {0} too short for length {1}'.format(\n",
        "            t.shape[0], seq_len))\n",
        "  x_seqs = []\n",
        "  y_seqs = []\n",
        "  t_rel = tf.slice(t, begin=(0, 0), size=(t.shape[0], t.shape[1] - 2))\n",
        "  for i in range(0, t.shape[0] - seq_len):\n",
        "    new_seq = tf.slice(t, begin=[i, 0], size=[seq_len, t.shape[1]])\n",
        "    rel_only = tf.slice(\n",
        "      new_seq,\n",
        "      begin=(0, 0),\n",
        "      size=(new_seq.shape[0], new_seq.shape[1] - 2)\n",
        "    )\n",
        "    x = tf.reshape(new_seq, shape=(new_seq.shape[0] * new_seq.shape[1],))\n",
        "    y = tf.reshape(rel_only, shape=(rel_only.shape[0] * rel_only.shape[1],))\n",
        "    x_seqs.append(x)\n",
        "    y_seqs.append(y)\n",
        "  x_seqs = tf.stack(x_seqs, axis=0)\n",
        "  y_seqs = tf.stack(y_seqs, axis=0)\n",
        "  return x_seqs, y_seqs\n",
        "\n",
        "\n",
        "def file_to_train_seqs(path, seq_len):\n",
        "  file_data = read_file(path)\n",
        "  trimmed_data = trim_abs_data(file_data)\n",
        "  accum_x = []\n",
        "  accum_y = []\n",
        "  inverted_drawings = invert_abs_data(trimmed_data)\n",
        "  scaled_drawings = []\n",
        "  scale_values = [0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.6, 0.5]\n",
        "#   scale_values = [0.9]\n",
        "  for d in inverted_drawings:\n",
        "    scaled_drawings += scale_drawing(d, scale_values)\n",
        "  for d in scaled_drawings:\n",
        "    drawing_tensor = abs_data_to_delta_tensor(d)\n",
        "    x_seqs, y_seqs = samples_to_sequences(drawing_tensor, seq_len)\n",
        "    accum_x.append(x_seqs)\n",
        "    accum_y.append(y_seqs)\n",
        "  return accum_x, accum_y\n",
        "\n",
        "\n",
        "class RnnLenException(Exception):\n",
        "  pass\n",
        "\n",
        "\n",
        "def seqs_to_rnn_seqs(x, y, seq_len):\n",
        "  sample_len = int(x.shape[1] / 4)\n",
        "  y_start_point = seq_len * sample_len\n",
        "  x_stop_point = x.shape[0] - y_start_point\n",
        "  if x_stop_point <= 0:\n",
        "    raise RnnLenException(\n",
        "      '{0} too short for RNN seq {1} ({2} seqs)'.format(\n",
        "          x.shape[0], seq_len, y_start_point))\n",
        "  x_seqs = []\n",
        "  for i in range(0, x_stop_point):\n",
        "    this_seq = []\n",
        "    for n in range(0, seq_len):\n",
        "      x_sl = tf.slice(\n",
        "        x, begin=(i + (n * sample_len), 0), size=(1, x.shape[1]))\n",
        "      this_seq.append(x_sl)\n",
        "    this_seq = tf.concat(this_seq, axis=0)\n",
        "    x_seqs.append(this_seq)\n",
        "  x_seqs = tf.stack(x_seqs, axis=0)\n",
        "  y_seqs = tf.slice(\n",
        "    y, begin=(y_start_point, 0), size=(y.shape[0] - y_start_point, y.shape[1])\n",
        "  )\n",
        "  assert x_seqs.shape[0] == y_seqs.shape[0], 'RNN: {0}!={1}'.format(\n",
        "    x_seqs.shape[0], y_seqs.shape[0])\n",
        "  return x_seqs, y_seqs\n",
        "\n",
        "\n",
        "def data_to_train_seqs(data_paths, seq_len, rnn_len=None):\n",
        "  accum_x_seqs = []\n",
        "  accum_y_seqs = []\n",
        "  accum_x_rnn = []\n",
        "  accum_y_rnn = []\n",
        "  for (i, path) in enumerate(data_paths):\n",
        "    if i % 10 == 0:\n",
        "      print('Parsing', i, '/', len(data_paths))\n",
        "    try:\n",
        "      x_seq_list, y_seq_list = file_to_train_seqs(path, seq_len)\n",
        "      for i in range(len(x_seq_list)):\n",
        "        x_seq = x_seq_list[i]\n",
        "        y_seq = y_seq_list[i]\n",
        "        accum_x_seqs.append(x_seq)\n",
        "        accum_y_seqs.append(y_seq)\n",
        "      if rnn_len:\n",
        "        for i in range(len(x_seq_list)):\n",
        "          x_rnn, y_rnn = seqs_to_rnn_seqs(x_seq, y_seq, rnn_len)\n",
        "          accum_x_rnn.append(x_rnn)\n",
        "          accum_y_rnn.append(y_rnn)\n",
        "    except (SeqLenException, RnnLenException) as e:\n",
        "      print('\\t{0} [file: {1}]'.format(e, path.split('/')[-1]))\n",
        "      continue\n",
        "  x = tf.concat(accum_x_seqs, axis=0)\n",
        "  y = tf.concat(accum_y_seqs, axis=0)\n",
        "  x_rnn, y_rnn = (None, None)\n",
        "  if rnn_len:\n",
        "    x_rnn = tf.concat(accum_x_rnn, axis=0)\n",
        "    y_rnn = tf.concat(accum_y_rnn, axis=0)\n",
        "  return (x, y), (x_rnn, y_rnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m6zn9zNMIph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequence_len = 16  # in the end will create N * num_samples per example\n",
        "train_rnn_len = 8\n",
        "\n",
        "training_sub_models = False\n",
        "\n",
        "doodle_csv_folder = os.path.join(drive_data_folder, 'train-data')\n",
        "doodle_file_paths = [\n",
        "    os.path.join(doodle_csv_folder, f)\n",
        "    for f in os.listdir(doodle_csv_folder)\n",
        "    if '.csv' in f\n",
        "]\n",
        "# doodle_file_paths = doodle_file_paths[:12]\n",
        "print('Found', len(doodle_file_paths), 'CSV files')\n",
        "\n",
        "(train_x, train_y), (train_rnn_x, train_rnn_y) = data_to_train_seqs(\n",
        "  doodle_file_paths, train_sequence_len, train_rnn_len)\n",
        "\n",
        "copy_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "if train_rnn_len:\n",
        "  rnn_dataset = tf.data.Dataset.from_tensor_slices((train_rnn_x, train_rnn_y))\n",
        "  # create a Y part of a dataset to copy RNN sequences\n",
        "  train_rnn_y_copy = tf.reshape(\n",
        "    train_rnn_x,\n",
        "    shape=(\n",
        "      train_rnn_x.shape[0],\n",
        "      train_rnn_x.shape[1],\n",
        "      int(train_rnn_x.shape[2] / 4),\n",
        "      4\n",
        "    )\n",
        "  )\n",
        "  train_rnn_y_copy = tf.slice(\n",
        "    train_rnn_y_copy,\n",
        "    begin=(0, 0, 0, 0),\n",
        "    size=(\n",
        "      train_rnn_y_copy.shape[0],\n",
        "      train_rnn_y_copy.shape[1],\n",
        "      train_rnn_y_copy.shape[2],\n",
        "      2\n",
        "    )\n",
        "  )\n",
        "  train_rnn_y_copy = tf.reshape(\n",
        "    train_rnn_y_copy,\n",
        "    shape=(\n",
        "      train_rnn_y_copy.shape[0],\n",
        "      train_rnn_y_copy.shape[1],\n",
        "      train_rnn_y_copy.shape[2] * train_rnn_y_copy.shape[3]\n",
        "    )\n",
        "  )\n",
        "  rnn_copy_dataset = tf.data.Dataset.from_tensor_slices((train_rnn_x, train_rnn_y_copy))\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Train X:', train_x.shape)\n",
        "print('Train Y:', train_y.shape)\n",
        "if train_rnn_len:\n",
        "  print('Train RNN X:', train_rnn_x.shape)\n",
        "  print('Train RNN Y:', train_rnn_y.shape)\n",
        "  print('Train RNN Y Copy:', train_rnn_y_copy.shape)\n",
        "print('Example Sample X:')\n",
        "print(train_x.numpy()[0][:8])\n",
        "print('Example Sample Y:')\n",
        "print(train_y.numpy()[0][:4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2BNbpMRQD2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def insert_abs_points(points, start_abs=None):\n",
        "  if start_abs is None:\n",
        "    start_abs = tf.constant([0, 0], dtype='float32')\n",
        "  start_abs = start_abs.numpy()\n",
        "  if len(start_abs) == 4:\n",
        "    start_abs = [start_abs[2] - start_abs[0], start_abs[3] - start_abs[1]]\n",
        "  rp = points.numpy()\n",
        "  ap = [rp[0], rp[1], rp[0] + start_abs[0], rp[1] + start_abs[1]]\n",
        "  for i in range(2, len(rp), 2):\n",
        "    abs_x = rp[i] + ap[-2]\n",
        "    abs_y = rp[i + 1] + ap[-1]\n",
        "    ap.append(rp[i])\n",
        "    ap.append(rp[i + 1])\n",
        "    ap.append(abs_x)\n",
        "    ap.append(abs_y)\n",
        "  return tf.constant(ap, shape=(len(ap)), dtype='float32')\n",
        "\n",
        "\n",
        "def get_sequence(seq_data, start_index):\n",
        "  if len(seq_data.shape) == 2:\n",
        "    ps = tf.slice(\n",
        "      seq_data,\n",
        "      [start_index, 0],\n",
        "      [1, seq_data.shape[1]]\n",
        "    )\n",
        "  else:\n",
        "    ps = tf.slice(\n",
        "      seq_data,\n",
        "      [start_index, 0, 0],\n",
        "      [1, seq_data.shape[1], seq_data.shape[2]]\n",
        "    )\n",
        "  return tf.squeeze(ps, axis=0)\n",
        "\n",
        "\n",
        "def get_random_sequence(seq_data):\n",
        "  start_index = random.randint(0, seq_data.shape[0])\n",
        "  return get_sequence(seq_data, start_index)\n",
        "\n",
        "\n",
        "def plot_points(points, color='black'):\n",
        "  points = points.numpy()\n",
        "  points = [\n",
        "    [points[i], points[i + 1]]\n",
        "    for i in range(2, len(points), 4)\n",
        "  ]\n",
        "  for i in range(1, len(points)):\n",
        "    plt.plot(\n",
        "      [points[i - 1][0], points[i][0]],\n",
        "      [points[i - 1][1], points[i][1]],\n",
        "      color=color\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_x_y(x, y, color='red'):\n",
        "  plot_points(x, color='black')\n",
        "  first_x_point = tf.slice(x, begin=(0,), size=(4,))\n",
        "  draw_y = insert_abs_points(y, first_x_point)\n",
        "  plot_points(draw_y, color=color)\n",
        "\n",
        "    \n",
        "def plot_rnn_x_y(x, y, color='red'):\n",
        "  x = tf.reshape(\n",
        "    x, shape=(x.shape[0] * x.shape[1],)\n",
        "  )\n",
        "  plot_points(x, color='black')\n",
        "  last_rel_point = tf.slice(x, begin=(x.shape[0] - 4,), size=(2,))\n",
        "  last_abs_point = tf.slice(x, begin=(x.shape[0] - 4,), size=(4,))\n",
        "  y = tf.concat([last_rel_point, y], axis=0)\n",
        "  y = insert_abs_points(y, last_abs_point)\n",
        "  plot_points(y, color=color)\n",
        "\n",
        "\n",
        "def draw_plot():\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "offset = random.randint(0, train_x.shape[0])\n",
        "for i in range(3):\n",
        "  draw_x = get_sequence(train_x, i + offset)\n",
        "  draw_y = get_sequence(train_y, i + offset)\n",
        "  plot_x_y(draw_x, draw_y)\n",
        "  draw_plot()\n",
        "\n",
        "if train_rnn_len:\n",
        "  for i in range(3):\n",
        "    offset = random.randint(0, train_rnn_x.shape[0])\n",
        "    draw_rnn_x = get_sequence(train_rnn_x, offset)\n",
        "    draw_rnn_y = get_sequence(train_rnn_y, offset)\n",
        "    plot_rnn_x_y(draw_rnn_x, draw_rnn_y)\n",
        "    draw_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWeSRb2xZU2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr37QcucWP9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_input_dims = train_sequence_len * 4\n",
        "num_output_dims = train_sequence_len * 2\n",
        "\n",
        "num_conv_layers = 3\n",
        "conv_filters_by_layer = [\n",
        "  32 * (2 ** n)\n",
        "  for n in range(num_conv_layers)\n",
        "]\n",
        "conv_settings = {\n",
        "  'kernel_size': (1, 5),\n",
        "  'strides': (1, 2),\n",
        "  'padding': 'same',\n",
        "  'activation': 'tanh',\n",
        "  'use_bias': True\n",
        "}\n",
        "\n",
        "num_latent_dims = int((train_sequence_len / (2 ** num_conv_layers)) * conv_filters_by_layer[-1])\n",
        "\n",
        "num_lstm_units = 256\n",
        "rnn_input_shape = (\n",
        "  train_rnn_len,\n",
        "  int(train_sequence_len / (2 ** num_conv_layers)),\n",
        "  conv_filters_by_layer[-1]\n",
        ")\n",
        "\n",
        "num_mdn_mixes = 16\n",
        "mdn_loss_func = mdn.get_mixture_loss_func(num_output_dims, num_mdn_mixes)\n",
        "\n",
        "\n",
        "def create_encoder():\n",
        "  input_layer = Input(shape=(num_input_dims,), dtype='float32')\n",
        "  x = input_layer\n",
        "  \n",
        "  # CONV LAYERS\n",
        "  x = layers.Reshape((1, train_sequence_len, 4))(x)  # height = 1\n",
        "  for n in range(num_conv_layers):\n",
        "    x = layers.Conv2D(filters=conv_filters_by_layer[n], **conv_settings)(x)\n",
        "\n",
        "  # FLATTEN\n",
        "  output_layer = layers.Flatten()(x)\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def create_rnn_encoder():\n",
        "  input_layer = Input(shape=(train_rnn_len, num_input_dims), dtype='float32')\n",
        "  x = input_layer\n",
        "  \n",
        "  # CONV LAYERS\n",
        "  x = layers.Reshape((train_rnn_len, train_sequence_len, 4))(x)  # height = 8\n",
        "  for n in range(num_conv_layers):\n",
        "    x = layers.Conv2D(filters=conv_filters_by_layer[n], **conv_settings)(x)\n",
        "  \n",
        "  output_layer = layers.Reshape((train_rnn_len, x.shape[-2] * x.shape[-1]))(x)\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def create_rnn_lstm():\n",
        "  input_layer = Input(shape=(train_rnn_len, num_latent_dims), dtype='float32')\n",
        "  x = input_layer\n",
        "\n",
        "  # RNN LAYERS\n",
        "  x = layers.LSTM(num_lstm_units, return_sequences=True)(x)\n",
        "  x = layers.LSTM(num_lstm_units, return_sequences=False)(x)\n",
        "  \n",
        "  # MDN LAYERS\n",
        "  output_layer = layers.Dense(num_latent_dims, activation='tanh')(x)\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def create_decoder():\n",
        "  input_layer = Input(shape=(num_latent_dims,), dtype='float32')\n",
        "  x = input_layer\n",
        "  \n",
        "  # CONV LAYERS\n",
        "  x = layers.Reshape((\n",
        "    1,\n",
        "    int(num_latent_dims / conv_filters_by_layer[-1]),\n",
        "    conv_filters_by_layer[-1])\n",
        "  )(x)  # height = 1\n",
        "  for n in range(1, num_conv_layers):\n",
        "    x = layers.Conv2DTranspose(filters=conv_filters_by_layer[::-1][n], **conv_settings)(x)\n",
        "  x = layers.Conv2DTranspose(filters=2, **conv_settings)(x)\n",
        "\n",
        "  # FLATTEN\n",
        "  output_layer = layers.Flatten()(x)\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def create_rnn_decoder():\n",
        "  input_layer = Input(shape=(train_rnn_len, num_latent_dims), dtype='float32')\n",
        "  x = input_layer\n",
        "  \n",
        "  # CONV LAYERS\n",
        "  x = layers.Reshape((\n",
        "    train_rnn_len,\n",
        "    int(num_latent_dims / conv_filters_by_layer[-1]),\n",
        "    conv_filters_by_layer[-1])\n",
        "  )(x)  # height = 1\n",
        "  for n in range(1, num_conv_layers):\n",
        "    x = layers.Conv2DTranspose(filters=conv_filters_by_layer[::-1][n], **conv_settings)(x)\n",
        "  x = layers.Conv2DTranspose(filters=2, **conv_settings)(x)\n",
        "\n",
        "  # FLATTEN\n",
        "  output_layer = layers.Reshape((train_rnn_len, num_output_dims))(x)\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def create_mdn_decoder():\n",
        "  input_layer = Input(shape=num_latent_dims, dtype='float32')\n",
        "  \n",
        "  # MDN LAYERS\n",
        "  output_layer = mdn.create_mdn_layers(\n",
        "      input_layer, num_output_dims, num_mdn_mixes, mu_activation='tanh')\n",
        "  return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "def mdn_predict(model, x, **kwargs):\n",
        "  while len(model.input_shape) > len(x.shape):\n",
        "    x = tf.expand_dims(x, axis=0)\n",
        "  y = model.predict(x)\n",
        "  y = tf.squeeze(y)\n",
        "  mdn_y = mdn.sample_from_output(\n",
        "    y, num_output_dims, num_mdn_mixes, **kwargs)\n",
        "  return tf.squeeze(tf.constant(mdn_y, shape=mdn_y.shape, dtype='float32'))\n",
        "\n",
        "\n",
        "def rnn_random_input(model, x, y, mdn=False):\n",
        "  rand_index = random.randint(0, x.shape[0])\n",
        "  seed = get_sequence(x, rand_index)\n",
        "  true_rnn_y = get_sequence(y, rand_index)\n",
        "  plot_rnn_x_y(seed, true_rnn_y, color='grey')\n",
        "  if mdn:\n",
        "    colors = ['red', 'orange', 'green', 'blue']\n",
        "    sigma_temps = [0.05, 0.005, 0.001, 0.0001]\n",
        "    for c, st in zip(colors, sigma_temps):\n",
        "      guess = mdn_predict(model, seed, temp=1, sigma_temp=st)\n",
        "      plot_rnn_x_y(seed, guess, color=c)\n",
        "  else:\n",
        "    guess = model.predict(tf.expand_dims(seed, axis=0))\n",
        "    guess = tf.squeeze(guess, axis=0)\n",
        "    plot_rnn_x_y(seed, guess, color='red')\n",
        "  draw_plot()\n",
        "  \n",
        "\n",
        "def copy_random_input(model, x, y, mdn=False):\n",
        "  rand_index = random.randint(0, x.shape[0])\n",
        "  seed = get_sequence(x, rand_index)\n",
        "  if mdn:\n",
        "    colors = ['red', 'orange', 'green']\n",
        "    sigma_temps = [0.005, 0.001, 0.0001]\n",
        "    for c, st in zip(colors, sigma_temps):\n",
        "      guess = mdn_predict(model, seed, temp=1, sigma_temp=st)\n",
        "      plot_x_y(seed, guess, color=c)\n",
        "  else:\n",
        "    guess = model.predict(tf.expand_dims(seed, axis=0))\n",
        "    guess = tf.squeeze(guess, axis=0)\n",
        "    if len(seed.shape) == 2:\n",
        "      seed = tf.reshape(seed, shape=(seed.shape[0] * seed.shape[1],))\n",
        "    if len(guess.shape) == 2:\n",
        "      guess = tf.reshape(guess, shape=(guess.shape[0] * guess.shape[1],))\n",
        "    plot_x_y(seed, guess)\n",
        "  draw_plot()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElwDj8zxlUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file_name = 'doodle_AAA_{0}_{1}_{2}_seq'.format(\n",
        "  '{0}', train_sequence_len, train_rnn_len\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUsq0mQ2dQIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loading_latest = True\n",
        "\n",
        "if loading_latest:\n",
        "  model_encoder = load_latest_model(model_file_name.format('encoder'))\n",
        "  model_decoder = load_latest_model(model_file_name.format('decoder'))\n",
        "else:\n",
        "  model_encoder = create_encoder()\n",
        "  model_decoder = create_decoder()\n",
        "\n",
        "print('Encoder')\n",
        "model_encoder.summary()\n",
        "print('\\n\\nDecoder')\n",
        "model_decoder.summary()\n",
        "\n",
        "model_copy = Sequential([model_encoder, model_decoder])\n",
        "print('\\n\\n')\n",
        "model_copy.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiIzZc6RnNlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  copy_random_input(model_copy, train_x, train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4V5TGzPKXLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  model_copy.compile(\n",
        "    loss='mse',\n",
        "    optimizer=optimizers.Adam(\n",
        "      learning_rate=0.0003\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDhoqlIsKj0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  num_epochs = 3\n",
        "  batch_size = 64\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    print()\n",
        "    print('Epoch {0}/{1}:'.format(i + 1, num_epochs))\n",
        "    this_dataset = copy_dataset.shuffle(train_x.shape[0]).batch(batch_size)\n",
        "    model_copy.fit(this_dataset, epochs=1, verbose=1)\n",
        "    for i in range(3):\n",
        "      copy_random_input(model_copy, train_x, train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrepxLZ9fQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  save_model(model_encoder, model_file_name.format('encoder'))\n",
        "  save_model(model_decoder, model_file_name.format('decoder'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RDAP4IbHoq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loading_latest = True\n",
        "\n",
        "if loading_latest:\n",
        "  model_rnn_encoder = load_latest_model(model_file_name.format('rnn_encoder'))\n",
        "  model_rnn_decoder = load_latest_model(model_file_name.format('rnn_decoder'))\n",
        "else:\n",
        "  model_rnn_encoder = create_rnn_encoder()\n",
        "  model_rnn_decoder = create_rnn_decoder()\n",
        "\n",
        "print('Encoder')\n",
        "model_rnn_encoder.summary()\n",
        "print('\\n\\nDecoder')\n",
        "model_rnn_decoder.summary()\n",
        "\n",
        "model_rnn_copy = Sequential([model_rnn_encoder, model_rnn_decoder])\n",
        "print('\\n\\n')\n",
        "model_rnn_copy.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVUlhUPG2IYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  copy_random_input(model_rnn_copy, train_rnn_x, train_rnn_y_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR6uDa36364n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  model_rnn_copy.compile(\n",
        "    loss='mse',\n",
        "    optimizer=optimizers.Adam(\n",
        "      learning_rate=0.0003\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVOXtmvH3MeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  num_epochs = 4\n",
        "  batch_size = 64\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    print()\n",
        "    print('Epoch {0}/{1}:'.format(i + 1, num_epochs))\n",
        "    this_dataset = rnn_copy_dataset.shuffle(train_x.shape[0]).batch(batch_size)\n",
        "    model_rnn_copy.fit(this_dataset, epochs=1, verbose=1)\n",
        "    for i in range(3):\n",
        "      copy_random_input(model_rnn_copy, train_rnn_x, train_rnn_y_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N38HHXJP35c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  save_model(model_rnn_encoder, model_file_name.format('rnn_encoder'))\n",
        "  save_model(model_rnn_decoder, model_file_name.format('rnn_decoder'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECsjn51w4XY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loading_latest = True\n",
        "\n",
        "if loading_latest:\n",
        "  model_mdn_decoder = load_latest_model(model_file_name.format('mdn_decoder'),{\n",
        "    'elu_plus_one_plus_epsilon': mdn.elu_plus_one_plus_epsilon\n",
        "  })\n",
        "else:\n",
        "  model_mdn_decoder = create_mdn_decoder()\n",
        "\n",
        "print('MDN Decoder')\n",
        "model_mdn_decoder.summary()\n",
        "\n",
        "model_encoder.trainable = False\n",
        "\n",
        "model_mdn_copy = Sequential([model_encoder, model_mdn_decoder])\n",
        "print('\\n\\n')\n",
        "model_mdn_copy.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ1agc_c4kFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  copy_random_input(model_mdn_copy, train_x, train_y, mdn=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpvW3ZUy5t7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  model_mdn_copy.compile(\n",
        "    loss=mdn_loss_func,\n",
        "    optimizer=optimizers.Adam(\n",
        "      learning_rate=0.0003\n",
        "    )\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKRyVm_86PHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  num_epochs = 2\n",
        "  batch_size = 64\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    print()\n",
        "    print('Epoch {0}/{1}:'.format(i + 1, num_epochs))\n",
        "    this_dataset = copy_dataset.shuffle(train_x.shape[0]).batch(batch_size)\n",
        "    model_mdn_copy.fit(this_dataset, epochs=1, verbose=1)\n",
        "    for i in range(3):\n",
        "      copy_random_input(model_mdn_copy, train_x, train_y, mdn=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLhNc1oV6Z22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if training_sub_models:\n",
        "  save_model(model_mdn_decoder, model_file_name.format('mdn_decoder'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RnMMbu7YDwoI",
        "colab": {}
      },
      "source": [
        "loading_latest = True\n",
        "\n",
        "if loading_latest:\n",
        "  model_rnn_lstm = load_latest_model(model_file_name.format('rnn_lstm'))\n",
        "else:\n",
        "  model_rnn_lstm = create_rnn_lstm()\n",
        "\n",
        "print('RNN LSTM')\n",
        "model_rnn_lstm.summary()\n",
        "\n",
        "model_rnn_encoder.trainable = False\n",
        "model_mdn_decoder.trainable = False\n",
        "\n",
        "model_rnn_mdn = Sequential([model_rnn_encoder, model_rnn_lstm, model_mdn_decoder])\n",
        "print('\\n\\n')\n",
        "model_rnn_mdn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZHV7PXw-yW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  rnn_random_input(model_rnn_mdn, train_rnn_x, train_rnn_y, mdn=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNZlc_-Y-4X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rnn_mdn.compile(\n",
        "  loss=mdn_loss_func,\n",
        "  optimizer=optimizers.Adam(\n",
        "    learning_rate=0.0003\n",
        "  )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2tZ2zln_hx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  print()\n",
        "  print('Epoch {0}/{1}:'.format(i + 1, num_epochs))\n",
        "  this_dataset = rnn_dataset.shuffle(train_x.shape[0]).batch(batch_size)\n",
        "  model_rnn_mdn.fit(this_dataset, epochs=1, verbose=1)\n",
        "  for i in range(3):\n",
        "    rnn_random_input(model_rnn_mdn, train_rnn_x, train_rnn_y, mdn=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFUVPs6h_oXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(model_rnn_lstm, model_file_name.format('rnn_lstm'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjvrfLRmBesS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(model_rnn_mdn, model_file_name.format('rnn_mdn'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}