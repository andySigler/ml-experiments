{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vox_decoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLiFTvhePXac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-2.0-preview --upgrade\n",
        "!pip install tf-nightly-gpu-2.0-preview --upgrade\n",
        "!pip install tfp-nightly --upgrade\n",
        "\n",
        "!pip install git+git://github.com/andysigler/keras-mdn-layer.git@functional-mdn-layers#egg=keras-mdn-layer --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL1xZpm33jrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow Version: ', tf.__version__)\n",
        "print('GPU: ', tf.test.gpu_device_name())\n",
        "from tensorflow.keras import Sequential, layers, models, Input, optimizers\n",
        "\n",
        "import mdn\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive_dest_folder = '/content/gdrive/My Drive/Colab Notebooks/data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gNuFQBgTM_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length of one sample\n",
        "sample_length = 32\n",
        "\n",
        "data_filename = 'david_long_19_5_29_15_40_00.csv';\n",
        "data_filename = os.path.join(drive_dest_folder, data_filename)\n",
        "max_data = 100000\n",
        "display_interval = 5000\n",
        "\n",
        "\n",
        "def normalize_vox_data(data):\n",
        "  s0, s1 = data.shape\n",
        "  spectrum_adder = tf.concat(\n",
        "      [\n",
        "        tf.zeros((s0, 1), dtype=tf.dtypes.float32),\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32) * 100\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  spectrum_divider = tf.concat(\n",
        "      [\n",
        "        tf.ones((s0, 1), dtype=tf.dtypes.float32),\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32) * 100\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  pitch_divider = tf.concat(\n",
        "      [\n",
        "        tf.ones((s0, 1), dtype=tf.dtypes.float32) * 1000,\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32)\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  return tf.divide(\n",
        "      tf.divide(\n",
        "          tf.add(data, spectrum_adder),\n",
        "          spectrum_divider\n",
        "      ),\n",
        "      pitch_divider\n",
        "  )\n",
        "\n",
        "\n",
        "def denormalize_vox_data(data):\n",
        "  s0, s1 = data.shape\n",
        "  spectrum_subtractor = tf.concat(\n",
        "      [\n",
        "        tf.zeros((s0, 1), dtype=tf.dtypes.float32),\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32) * 100\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  spectrum_multiplier = tf.concat(\n",
        "      [\n",
        "        tf.ones((s0, 1), dtype=tf.dtypes.float32),\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32) * 100\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  pitch_multiplier = tf.concat(\n",
        "      [\n",
        "        tf.ones((s0, 1), dtype=tf.dtypes.float32) * 1000,\n",
        "        tf.ones((s0, s1 - 1), dtype=tf.dtypes.float32)\n",
        "      ],\n",
        "      1\n",
        "  )\n",
        "  return tf.subtract(\n",
        "      tf.multiply(\n",
        "          tf.multiply(data, pitch_multiplier),\n",
        "          spectrum_multiplier),\n",
        "      spectrum_subtractor\n",
        "  )\n",
        "\n",
        "\n",
        "def load_csv_to_tensor(max_samples=max_data):\n",
        "  accumulated_data = None\n",
        "  with open(data_filename, 'r') as f:\n",
        "    features = f.readline().strip().split(',');\n",
        "    print('Read in features:', features)\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      line = [float(val.strip()) for val in line.split(',') if val.strip()]\n",
        "      line = tf.constant([line], dtype=tf.dtypes.float32)\n",
        "      if accumulated_data is None:\n",
        "        accumulated_data = line\n",
        "      else:\n",
        "        accumulated_data = tf.concat([accumulated_data, line], 0)\n",
        "      if accumulated_data.shape[0] % display_interval == 0:\n",
        "        print('Read in {} lines from CSV'.format(accumulated_data.shape[0]))\n",
        "      if accumulated_data.shape[0] >= max_samples:\n",
        "        break\n",
        "  return accumulated_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNw6hlq-fUkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_tensor = load_csv_to_tensor()\n",
        "csv_tensor = normalize_vox_data(csv_tensor)\n",
        "print('Parsed', csv_tensor.shape[0], 'lines samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVCBZlgHOTo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_parse_interval = 10000\n",
        "\n",
        "\n",
        "def generate_training_sequences(data, x_length, y_length, y_offset):\n",
        "  all_inputs = None\n",
        "  all_outputs = None\n",
        "  max_index = data.shape[0] - max(y_offset + y_length, x_length)\n",
        "  for start_index in range(max_index):\n",
        "    if start_index % display_parse_interval == 0:\n",
        "      print('Parsing', start_index, '/', max_index)\n",
        "    input_seq = tf.slice(\n",
        "        data,\n",
        "        [start_index, 0],\n",
        "        [x_length, data.shape[1]]\n",
        "    )\n",
        "    input_seq = tf.expand_dims(input_seq, 0)\n",
        "    output_seq = tf.slice(\n",
        "        data,\n",
        "        [start_index + y_offset, 0],\n",
        "        [y_length, data.shape[1]]\n",
        "    )\n",
        "    output_seq = tf.expand_dims(output_seq, 0)\n",
        "    if all_inputs is None:\n",
        "      all_inputs = input_seq\n",
        "      all_outputs = output_seq\n",
        "    else:\n",
        "      all_inputs = tf.concat([all_inputs, input_seq], 0)\n",
        "      all_outputs = tf.concat([all_outputs, output_seq], 0)\n",
        "  return all_inputs, all_outputs\n",
        "\n",
        "\n",
        "def remove_silence(data_tensor):\n",
        "  new_data_tensor = None\n",
        "  def _add_value(np_val):\n",
        "    nonlocal new_data_tensor\n",
        "    new_tensor = tf.expand_dims(tf.constant(np_val), 0)\n",
        "    if new_data_tensor is None:\n",
        "      new_data_tensor = new_tensor\n",
        "    else:\n",
        "      new_data_tensor = tf.concat([new_data_tensor, new_tensor], 0)\n",
        "\n",
        "  np_data = data_tensor.numpy()\n",
        "  for i in range(data_tensor.shape[0]):\n",
        "    if i % display_parse_interval == 0:\n",
        "      print('Removing Silence from', i)\n",
        "    if np_data[i][0][0] > 0:\n",
        "      # automatically use sample if there is pitch data\n",
        "      _add_value(np_data[i])\n",
        "    elif max(np_data[i][0]) > 0.75:\n",
        "      _add_value(np_data[i])\n",
        "  return new_data_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7cQWrsFfWm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_seq_length = 1\n",
        "\n",
        "# Autoencoder\n",
        "print('Parsing Autoencoder Train Data')\n",
        "train_autoencoder, _ = generate_training_sequences(\n",
        "  csv_tensor, train_seq_length, 1, 0)\n",
        "print('Autoencoder Data Shape:', train_autoencoder.shape)\n",
        "train_autoencoder = remove_silence(train_autoencoder)\n",
        "print('No-Silence Data Shape:', train_autoencoder.shape)\n",
        "train_data_autoencoder = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_autoencoder, train_autoencoder))\n",
        "total_train_examples_autoencoder = train_autoencoder.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrx6vrxl3jr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize predictions.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pixel_per_inch = 16\n",
        "\n",
        "\n",
        "def draw_spectrum(seq):\n",
        "  # time is left->right, lower frequencies at bottom of chart\n",
        "  # the lowest band is the pitch (0.0-1.0)\n",
        "#   seq = np.flip(np.rot90(seq, k=3), axis=1)\n",
        "  x_size = int(seq.shape[1] / pixel_per_inch)\n",
        "  y_size = int(seq.shape[0] / pixel_per_inch)\n",
        "#   plt.figure(figsize=(x_size, y_size))  # inches\n",
        "  plt.imshow(seq, origin=\"lower\", cmap=\"gray\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "print('RNN Data')\n",
        "for x, y in train_data_autoencoder.shuffle(1000).take(3):\n",
        "  draw_spectrum(tf.concat([x,y], 0).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--TkXKbB-WMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_conv_layers = 3\n",
        "num_conv_filters = 32\n",
        "num_conv_pixels = 3\n",
        "num_conv_steps = 1\n",
        "\n",
        "conv_to_flat_multiplier = sample_length\n",
        "for i in range(num_conv_layers):\n",
        "  conv_to_flat_multiplier /= num_conv_steps\n",
        "conv_to_flat_multiplier = int(conv_to_flat_multiplier)\n",
        "\n",
        "num_hidden = int(conv_to_flat_multiplier * num_conv_filters * 0.25)\n",
        "# num_hidden_2 = int(num_hidden_1 * 0.25)\n",
        "num_latent_units = 2\n",
        "\n",
        "\n",
        "def create_encoder():\n",
        "  # ENCODER\n",
        "  model = Sequential()\n",
        "  model.add(layers.Reshape(\n",
        "      (1, sample_length, 1),\n",
        "      input_shape=(1, sample_length)\n",
        "  ))\n",
        "  for i in range(num_conv_layers):\n",
        "      model.add(layers.Conv2D(\n",
        "          filters=num_conv_filters,\n",
        "          kernel_size=(1, num_conv_pixels),\n",
        "          padding='same',\n",
        "          strides=(1, num_conv_steps),\n",
        "          activation='relu'\n",
        "      ))\n",
        "  # LATENT SPACE\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(\n",
        "      num_hidden,\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(layers.Dense(\n",
        "      num_latent_units,\n",
        "      activation='sigmoid'\n",
        "  ))\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_decoder():\n",
        "  model = Sequential()\n",
        "  # LATENT SPACE\n",
        "  model.add(layers.Dense(\n",
        "      num_hidden,\n",
        "      activation='relu',\n",
        "      input_shape=(num_latent_units,)\n",
        "  ))\n",
        "  model.add(layers.Dense(\n",
        "      conv_to_flat_multiplier * num_conv_filters,\n",
        "      activation='relu'\n",
        "  ))\n",
        "  model.add(layers.Reshape(\n",
        "    (1, conv_to_flat_multiplier, num_conv_filters)\n",
        "  ))\n",
        "  # DECODER\n",
        "  for i in range(num_conv_layers):\n",
        "      model.add(layers.Conv2DTranspose(\n",
        "          filters=num_conv_filters,\n",
        "          kernel_size=(1, num_conv_pixels),\n",
        "          padding='same',\n",
        "          strides=(1, num_conv_steps),\n",
        "          activation='relu'\n",
        "      ))\n",
        "  model.add(layers.Conv2DTranspose(\n",
        "      filters=1,\n",
        "      kernel_size=(1, num_conv_pixels),\n",
        "      padding='same',\n",
        "      strides=(1, 1),\n",
        "      activation='sigmoid'\n",
        "  ))\n",
        "  model.add(layers.Reshape((1, sample_length)))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PSodoD4d6aJU",
        "colab": {}
      },
      "source": [
        "def dated_filename():\n",
        "  return str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
        "\n",
        "\n",
        "def save_model(model, name):\n",
        "  blob_name = '{0}_{1}.h5'.format(name, dated_filename())\n",
        "  print('Saving -> {}'.format(blob_name))\n",
        "  model.save(os.path.join(drive_dest_folder, blob_name))\n",
        "\n",
        "\n",
        "def load_latest_model(name):\n",
        "  model_files = [\n",
        "      f\n",
        "      for f in os.listdir(drive_dest_folder)\n",
        "      if name in f and 'h5' in f\n",
        "  ]\n",
        "  model_files.sort()\n",
        "  latest_file_name = model_files[-1]\n",
        "  latest_filepath = os.path.join(drive_dest_folder, latest_file_name)\n",
        "  print('Loading -> {}'.format(latest_filepath))\n",
        "  model = tf.keras.models.load_model(latest_filepath)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bz9O0H1igS2y",
        "colab": {}
      },
      "source": [
        "loading_latest = False\n",
        "\n",
        "if loading_latest:\n",
        "  model_encoder = load_latest_model('fuck_vox_encoder')\n",
        "  model_decoder = load_latest_model('fuck_vox_decoder')\n",
        "else:\n",
        "  model_encoder = create_encoder()\n",
        "  model_decoder = create_decoder()\n",
        "\n",
        "\n",
        "total_training_epochs_autoencoder = 0\n",
        "all_losses = []\n",
        "model_encoder.summary()\n",
        "model_decoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5EQktUc_qi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_autoencoder = Sequential([model_encoder, model_decoder])\n",
        "\n",
        "model_autoencoder.compile(\n",
        "    loss='mse',\n",
        "    optimizer=optimizers.Adam(learning_rate=0.0001))\n",
        "\n",
        "model_autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUuGAK7T_syk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_random_guesses_autoencoder(model, data, num_guess):\n",
        "  for x, _ in data.shuffle(total_train_examples_autoencoder).take(num_guess):\n",
        "    guess = model.predict(tf.expand_dims(x, 0))\n",
        "    guess = tf.squeeze(guess, 0)\n",
        "    combined = tf.concat([x, guess], 0)\n",
        "    draw_spectrum(combined.numpy())\n",
        "\n",
        "\n",
        "print_random_guesses_autoencoder(model_autoencoder, train_data_autoencoder, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyxZp2QCKjtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 16\n",
        "batch_size = 4\n",
        "training_samples = total_train_examples_autoencoder\n",
        "total_batches = int(training_samples / batch_size)\n",
        "display_step = int(total_batches / 10)\n",
        "\n",
        "temp_dataset = train_data_autoencoder.take(training_samples)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  tds = temp_dataset.shuffle(training_samples)\n",
        "  tds = tds.batch(batch_size).prefetch(1)\n",
        "  print(\n",
        "      'Epoch', epoch + 1, '/', num_epochs,\n",
        "      '(', total_training_epochs_autoencoder, 'total )'\n",
        "   )\n",
        "  for i, (batch_x, batch_y) in enumerate(tds.take(total_batches)):\n",
        "      loss = model_autoencoder.train_on_batch(batch_x, batch_y)\n",
        "      all_losses += [loss]\n",
        "      if (i * batch_size) % display_step == 0:\n",
        "        print(\n",
        "            '\\tBatch', i, '/', total_batches\n",
        "        )\n",
        "  plt.plot(all_losses)\n",
        "  plt.show()\n",
        "  print_random_guesses_autoencoder(\n",
        "      model_autoencoder, train_data_autoencoder, 5)\n",
        "  total_training_epochs_autoencoder += 1\n",
        "\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HVr9OLp-T_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(model_encoder, 'fuck_vox_encoder')\n",
        "save_model(model_decoder, 'fuck_vox_decoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI2iYV8vNSqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}