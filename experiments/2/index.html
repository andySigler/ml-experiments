<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <link rel="stylesheet" href="../../css/main.css">
        <link rel="icon" href="../../img/favicon.ico" type="image/x-icon">
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.1.2/dist/tf.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.0.1/Tone.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.8.0/p5.js"></script>
        <script type="text/javascript" src="./js/audioNodes.js"></script>
        <script type="text/javascript" src="./js/pitch.js"></script>
        <script type="text/javascript" src="./js/vocoder.js"></script>
        <script type="text/javascript" src="./js/data.js"></script>
        <script type="text/javascript" src="./js/ui.js"></script>
        <script type="text/javascript" src="./js/canvas.js"></script>
        <script type="text/javascript" src="./js/main.js"></script>
        <style type="text/css">
            canvas {
                border: 1px solid black;
                border-radius: 5px;
            }
            #faceGif {
                width: 300px;
            }
            img {
                border: 2px solid black;
                border-radius: 5px;
            }
        </style>
    </head>
    <body>
        <div class="container" id="container">
            <img src="./data/david-robot.jpg">
            <div>
                <h1>ML-Experiment: Vocoder Autencoder</h1>
                <p>
                    Below is a noise generating ... thing ... I made, which uses Tensorflow to play back vocal features through a vocoder, in real-time!
                </p>
                <p>
                    See the <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/2/js">JS source code here</a> (includes the TFJS inference code, plus a custom-built vocoder), and also the <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/2/python/vox_decoder.ipynb">Python notebook</a> used for training in Tensorflow-2.0-beta.
                </p>
                <br>
                <h2>Make "Throat Noises" with an XY Pad</h2>
                <img src="./data/face_massage.gif" id="faceGif">
                <ol>
                    <li>Press the "Start" button</li>
                    <li>Turn your computer's volume up</li>
                    <li>Move your cursor over the XY Pad</li>
                    <li>TIP: keep near the center and move quickly, to create more interesting sounds...</li>
                </ol>
                <button id="decoderButton" disabled="true">Instrument: Empty</button>
                <br />
                <br />
                <div id="canvas">
                </div>
            </div>
            <div>
                <br>
                <h2>The Goal</h2>
                <p>
                    My goal was to create a system, using an autoencoder, that can generate vocal features imn real-time (<a href="http://www.andysigler.com/projects/noser/">I have a history of liking throat noises</a>).
                </p>
                <p>
                    This system use a model, trained from the vocal features of an audio recording (I found a audio book of <a href="https://en.wikipedia.org/wiki/David_Attenborough">David Attenborough</a>)
                </p>
                <p>
                    You can listen to a small sampling from that original recording by pressing play below:
                </p>
                <audio controls>
                    <source src="./data/david_cropped.ogg" type="audio/ogg">
                    <source src="./data/david_cropped.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
                <p>
                    And below, here is a simple diagram showing that audio file being played:
                </p>
                <p>
                    <img src="./data/david-diagram-1.png">
                </p>
                <br>
                <h2>A Real-Time Vocoder</h2>
                <p>
                    Now, the problem with making a real-time interactive system, is that any example ML project I could find that trained from audio features was <strong>NOT</strong> even close to real-time (like the <a href="https://github.com/Rayhane-mamah/Tacotron-2">Tacotron-2</a>). This is because of the many data points needed to synthesis something like an FFT spectrum, let alone if it was actual audio samples.
                </p>
                <p>
                    To solve this problem, I decided to create a vocoder, which would greatly compress the number of data points needed, thus allowing it to be real-time.
                </p>
                <p>
                    Press the "Start" button below to hear what that sounds like:
                </p>
                <button id="mp3Button" disabled="true">Vocoder: Empty</button>
                <span style="display:none">
                    <input type="checkbox" id="mp3Record" disabled="true">
                    Record Data
                </span>
                <p>
                    The vocal features of the audio recording are picked up by a series of band-pass filters, which are then used to control the amplitude of more bandpass filters, modulating the timbre of the source square and noise generators. In addition, a simple pitch-detector is used to control the square wave's frequency.
                </p>
                <p>
                    See a diagram of how that works below:
                </p>
                <p>
                    <img src="./data/david-diagram-2.png">
                </p>
                <br>
                <h2>Training in Tensorflow</h2>
                <p>
                    To generate the data needed for training a model, I recording a couple hours of the vocoder's output, plus the detected pitch.
                </p>
                <p>
                    <img src="./data/david-diagram-3.png">
                </p>
                <p>
                    The train data could then be loaded into a <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/2/python/vox_decoder.ipynb">Python notebook</a>, and used to train a convolutional autoencoder.
                </p>
                <p>
                    <img src="./data/vox_autoencoder_drawing.jpg">
                </p>
                <p>
                    Once the training was done, the model is loaded back onto the browser using TensorflowJS, so that the XY pad can be used to control the band-pass filters' amplitudes, and the square wave frequency.
                </p>
                <p>
                    <img src="./data/david-diagram-4.png">
                </p>
            </div>
            <div>
                <h2>Did It Work?</h2>
                <p>
                    While I was able to make something that sounds throat-ish, my final model does not contain the level of spectral detail I would have wanted. It sounds mostly like gutteral whispering most of the time...
                </p>
                <p>
                    Some places for improvement are:
                    <ol>
                        <li>
                            Placing the vocoder (and training data) filter frequencies at the <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">mel-spectrogram frequencies</a>, rather than exponentially like I did. My implementation puts too much emphasis on higher frequencies, which are not as important as the lower ones in speech synthesis.
                        </li>
                        <li>
                            I forced myself to compress the latent space of my model to 2 variables, which might have been too small for the amount of detail I wanted. However, the interaction of a two-variable XY pad seemed a noble goal.
                        </li>
                        <li>
                            The architecture of my model could certainly be improved, as I'm operating with a lot of brand new information to me, plus I didn't want to spend too much time of this since I'm only just learning about ML. Maybe a GAN would create more detail in the output spectral features.
                        </li>
                    </ol>
                </p>
            </div>
        </div>
    </body>
</html>
