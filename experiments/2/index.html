<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <link rel="stylesheet" href="../../css/main.css">
        <link rel="icon" href="../../img/favicon.ico" type="image/x-icon">
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.1.2/dist/tf.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/13.0.1/Tone.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.8.0/p5.js"></script>
        <script type="text/javascript" src="./js/audioNodes.js"></script>
        <script type="text/javascript" src="./js/pitch.js"></script>
        <script type="text/javascript" src="./js/vocoder.js"></script>
        <script type="text/javascript" src="./js/data.js"></script>
        <script type="text/javascript" src="./js/ui.js"></script>
        <script type="text/javascript" src="./js/canvas.js"></script>
        <script type="text/javascript" src="./js/main.js"></script>
        <style type="text/css">
            canvas {
                border: 1px solid black;
                border-radius: 5px;
            }
            #faceGif {
                width: 300px;
            }
        </style>
    </head>
    <body>
        <div class="container" id="container">
            <h1>ML-Experiment #2</h1>
            <div>
                <h2>Trained Deep Model</h2>
                <p>
                    Below is a 2-valued (XY) compressed latent space (controlled by XY sliders) of the spectral and pitch features of <a href="https://en.wikipedia.org/wiki/David_Attenborough">David Attenborough</a>'s voice.
                </p>
                <ol>
                    <li>
                        Press the button to start the instrument
                    </li>
                    <li>
                        Drag your cursor around the XY pad to explore its learned spectral features
                    </li>
                    <li>
                        Press the button again to stop the instrument
                    </li>
                </ol>
                <button id="decoderButton" disabled="true">Instrument: Empty</button>
                <br />
                <br />
                <div id="canvas">
                </div>
            </div>
            <div>
                <h2>What Is Happening Here...</h2>
                <p>
                    A custom-built <a href="https://en.wikipedia.org/wiki/Vocoder">vocoder</a> is being used to generate what sounds like some strange vocal sounds. The vocoder filters' filter amplitudes are being directly controlled by the output from a deep model, trained on voice data. In addition, a square-wave oscillator is having its pitch controlled, to mimic the types of pitches heard in a voice.
                </p>
                <p>
                    To ensure that it would (kind of) work in the end, all training data was likewise generated by passing an audio sample through that same custom vocoder, and saving the detected frequency amplitudes.
                </p>
                <p>
                    Here is a <a href="https://github.com/andySigler/ml-experiments/tree/master/experiments/2/python/vox_decoder.ipynb">link to the python notebook</a> used to train autoencoder, and below is a drawing of the autoencoder's structure:
                </p>
                <p>
                    <img src="./data/vox_autoencoder_drawing.jpg">
                </p>
            </div>
            <div>
                <h2>My Goal...</h2>
                <img src="./data/face_massage.gif" id="faceGif">
                <p>
                    I wanted to make a deep model that learned the spectral shapes, and pitches, generated from a voice. Then, I thought, it would be fun to manipulate and explore that compressed latent space it in real-time.
                </p>
                <p>
                    I imagined it would be like I was taking the original speaker's mouth and vocal chords, and bending them around to make strange through noises (<a href="http://www.andysigler.com/projects/noser/">I have a history of liking throat noises</a>).
                </p>
                <p>
                    Here is a sample from the original audio recording I used to train from:
                </p>
                <audio controls>
                    <source src="./data/david_cropped.ogg" type="audio/ogg">
                    <source src="./data/david_cropped.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
            </div>
            <div>
                <h2>A Problem with Real-Time Interaction...</h2>
                <p>
                    While there are ML modelling techniques that can both learn and generate auditory features (from/to spectral information), I could not find any way to do this in "real-time".
                </p>
                <p>
                    The thing is, real-time is important to me, because at it's core I wanted to make an iteractive tool.
                </p>
                <p>
                    An example of <strong>non</strong>-real-time speec synthesis includes implementations of the <a href="https://github.com/Rayhane-mamah/Tacotron-2">Tacotron-2</a>, which uses spectral information (mel-spectrograms), to train a wavenet and text-to-speech. The Tacotron-2, however, is far from real-time, and requires a bit of time to generate the audio.
                </p>
            </div>
            <div>
                <h2>Vocoder as Solution for Real-Time</h2>
                <p>
                    While a vocoder might sound a tad too robotic, the amount of information required to make something sound <strong>close enough</strong> to a voice is far less than other methods.
                </p>
                <p>
                    So, I built a vocoder using <a href="https://tonejs.github.io/">ToneJS</a>, which can both handle an MP3 file as the modulating source, as well as a TFJS model to control the filters' amplitudes and oscillator pitch.
                </p>
                <p>
                    Click the button below to hear the original MP3 file (from above), being filtered through the vocoder. Sounds pretty robotic, eh?
                </p>
                <button id="mp3Button" disabled="true">Playback: Empty</button>
                <span style="display:none">
                    <input type="checkbox" id="mp3Record" disabled="true">
                    Record Data
                </span>
            </div>
            <div>
                <h2>Did It Work?</h2>
                <p>
                    While I was able to make something that sounds throat-ish, my final model does not contain the level of spectral detail I would have wanted. It sounds mostly like gutteral whispering most of the time...
                </p>
                <p>
                    Some places for improvement are:
                    <ol>
                        <li>
                            Placing the vocoder (and training data) filter frequencies at the <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">mel-spectrogram frequencies</a>, rather than exponentially like I did. My implementation puts too much emphasis on higher frequencies, which are not as important as the lower ones in speech synthesis.
                        </li>
                        <li>
                            I forced myself to compress the latent space of my model to 2 variables, which might have been too small for the amount of detail I wanted. However, the interaction of a two-variable XY pad seemed a noble goal.
                        </li>
                        <li>
                            The architecture of my model could certainly be improved, as I'm operating with a lot of brand new information to me, plus I didn't want to spend too much time of this since I'm only just learning about ML. Maybe a GAN would create more detail in the output spectral features.
                        </li>
                    </ol>
                </p>
            </div>
        </div>
    </body>
</html>
