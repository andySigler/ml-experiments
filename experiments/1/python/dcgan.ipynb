{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLiFTvhePXac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0 --upgrade\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0 --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL1xZpm33jrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow Version: ', tf.__version__)\n",
        "print('GPU: ', tf.test.gpu_device_name())\n",
        "from tensorflow.compat.v1.keras import Sequential, layers, models\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "drive_dest_folder = '/content/gdrive/My Drive/Colab Notebooks/data'\n",
        "\n",
        "# MNIST Dataset parameters.\n",
        "pixel_res = 28\n",
        "num_features = pixel_res * pixel_res # data features (img shape: 28*28).\n",
        "\n",
        "# Network parameters.\n",
        "noise_dim = num_features # Noise data points."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45C8SikXKY3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_model():\n",
        "    model = Sequential()\n",
        "    model.add(layers.Dense(4096, input_shape=(noise_dim,)))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.Dense(128*7*7))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
        "    model.add(layers.UpSampling2D(size=(2, 2)))\n",
        "    model.add(layers.Conv2D(64, (5, 5), padding='same'))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.UpSampling2D(size=(2, 2)))\n",
        "    model.add(layers.Conv2D(1, (5, 5), padding='same'))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def discriminator_model():\n",
        "    model = Sequential()\n",
        "    model.add(layers.Reshape(\n",
        "      target_shape=(pixel_res, pixel_res, 1),\n",
        "      input_shape=(pixel_res, pixel_res, 1)))\n",
        "    model.add(\n",
        "      layers.Conv2D(64, (5, 5), padding='same')\n",
        "    )\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Conv2D(128, (5, 5)))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1024))\n",
        "    model.add(layers.Activation('tanh'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.add(layers.Activation('sigmoid'))\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_combination(gen, dis):\n",
        "  new_model = tf.keras.Sequential()\n",
        "  new_model.add(gen)\n",
        "  dis.trainable = False\n",
        "  new_model.add(dis)\n",
        "  return new_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxtA37_h_dnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dated_filename():\n",
        "  return str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
        "\n",
        "\n",
        "def save_model(model, name):\n",
        "  blob_name = '{0}_{1}.h5'.format(name, dated_filename())\n",
        "  print('Saving -> {}'.format(blob_name))\n",
        "  model.save(os.path.join(drive_dest_folder, blob_name))\n",
        "\n",
        "\n",
        "def load_latest_model(name):\n",
        "  model_files = [\n",
        "      f\n",
        "      for f in os.listdir(drive_dest_folder)\n",
        "      if name in f and 'h5' in f\n",
        "  ]\n",
        "  model_files.sort()\n",
        "  latest_file_name = model_files[-1]\n",
        "  latest_filepath = os.path.join(drive_dest_folder, latest_file_name)\n",
        "  print('Loading -> {}'.format(latest_filepath))\n",
        "  model = tf.keras.models.load_model(latest_filepath)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxjmxnt4O0i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loading = True\n",
        "if loading:\n",
        "  generator = load_latest_model('generator')\n",
        "  discriminator = load_latest_model('discriminator')\n",
        "else:\n",
        "  generator = generator_model()\n",
        "  discriminator = discriminator_model()\n",
        "\n",
        "print(generator.output_shape)\n",
        "print(discriminator.input_shape)\n",
        "combined = create_combination(generator, discriminator)\n",
        "total_training_epochs = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyTGfBpYJQEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters.\n",
        "learning_rate = 0.0001\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnl5p2zn3jro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "#     optimizer=\"SGD\"\n",
        "    optimizer=tf.keras.optimizers.SGD(lr=learning_rate)\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
        ")\n",
        "\n",
        "discriminator.trainable = False\n",
        "combined.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(\n",
        "        lr=learning_rate, momentum=0.9, nesterov=True\n",
        "    )\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
        ")\n",
        "\n",
        "discriminator.trainable = True\n",
        "discriminator.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.SGD(\n",
        "        lr=learning_rate, momentum=0.9, nesterov=True\n",
        "    )\n",
        "#     optimizer=tf.keras.optimizers.Adam(learning_rate)\n",
        ")\n",
        "\n",
        "true_labels = tf.ones([batch_size], dtype=tf.float32)\n",
        "false_labels = tf.zeros([batch_size], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gNuFQBgTM_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Prepare MNIST data.\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Convert to float32.\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "# Normalize images value from [0, 255] to [0, 1].\n",
        "x_train, x_test = x_train / 255., x_test / 255.\n",
        "# reshape for the disciminator\n",
        "x_train = np.reshape(x_train, (-1, pixel_res, pixel_res, 1))\n",
        "x_test = np.reshape(x_test, (-1, pixel_res, pixel_res, 1))\n",
        "# Rescale to [-1, 1], the input range of the discriminator\n",
        "x_train = (x_train * 2) - 1\n",
        "x_test = (x_test * 2) - 1\n",
        "\n",
        "# Use tf.data API to shuffle and batch data.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(60000).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrx6vrxl3jr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize predictions.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def generate_sliding_noise(indexes_to_slide=[0]):\n",
        "  noise_line = np.random.normal(-1., 1., size=noise_dim).astype(np.float32)\n",
        "  noise = np.zeros([36, noise_dim]).astype(np.float32)\n",
        "  for i in range(noise.shape[0]):\n",
        "    noise[i] = noise_line\n",
        "    for n in indexes_to_slide:\n",
        "      noise[i][n] = ((i / (noise.shape[0] - 1)) * 2) - 1\n",
        "  return noise\n",
        "\n",
        "\n",
        "def draw_data(t):\n",
        "  # Rescale to original [0, 1]\n",
        "  t = (t + 1.) / 2\n",
        "  # Reverse colors for better display\n",
        "  t = -1 * (t - 1)\n",
        "  canvas = np.empty((pixel_res * n, pixel_res * n))\n",
        "  for i in range(n):\n",
        "      for j in range(n):\n",
        "          image_data = t[(i * n) + j].reshape([pixel_res, pixel_res])\n",
        "          canvas[i * pixel_res:(i + 1) * pixel_res, j * pixel_res:(j + 1) * pixel_res] = image_data\n",
        "  plt.figure(figsize=(n, n))\n",
        "  plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def draw_noise(noise_input):\n",
        "  # Generate image from noise.\n",
        "  guess = generator.predict(noise_input)\n",
        "  draw_data(guess)\n",
        "\n",
        "  \n",
        "def draw_from_dataset(data):\n",
        "  draw_data(data)\n",
        "\n",
        "\n",
        "# Generate images from noise, using the generator network.\n",
        "n = 6\n",
        "draw_noise(generate_sliding_noise([10,20,30,40,50,60, 70, 80,90]))\n",
        "z = np.random.normal(-1., 1., size=[n * n, noise_dim]).astype(np.float32)\n",
        "draw_noise(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tqZd6puP3jry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_steps = 10000\n",
        "display_step = 500\n",
        "\n",
        "\n",
        "def test_images():\n",
        "  z = np.random.normal(-1., 1., size=[n * n, noise_dim]).astype(np.float32)\n",
        "  draw_noise(z)\n",
        "\n",
        "# Run training for the given number of steps.\n",
        "for i, batch_x in enumerate(train_data.take(training_steps)):\n",
        "    \n",
        "    if i % display_step == 0:\n",
        "      print('On Batch ', i, '/', training_steps, '(', total_training_epochs, 'total )')\n",
        "      test_images()\n",
        "    \n",
        "    total_training_epochs += 1\n",
        "    \n",
        "    # Generate noise.\n",
        "    noise_input = np.random.normal(\n",
        "        -1., 1., size=[batch_size, noise_dim]).astype(np.float32)\n",
        "    # use noise to create fake images\n",
        "    fake_images = generator.predict(noise_input)\n",
        "    # train the discriminator, using both fake and real images\n",
        "    discriminator.trainable = True\n",
        "    discriminator.train_on_batch(fake_images, false_labels)\n",
        "    discriminator.train_on_batch(batch_x, true_labels)\n",
        "    \n",
        "    # Generate noise.\n",
        "    noise_input = np.random.normal(\n",
        "        -1., 1., size=[batch_size, noise_dim]).astype(np.float32)\n",
        "    discriminator.trainable = False\n",
        "    combined.train_on_batch(noise_input, true_labels)\n",
        "\n",
        "\n",
        "print('done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9F9pPRfEitH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(generator, 'generator')\n",
        "save_model(discriminator, 'discriminator')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sehTtUD7rrx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}