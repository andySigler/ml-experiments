{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpsons-gen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSslwTxVjcqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1 --upgrade\n",
        "!pip install tensorflow-gpu==2.0.0-beta1 --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sVM-L0Zjj_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow Version: ', tf.__version__)\n",
        "print('GPU: ', tf.test.gpu_device_name())\n",
        "from tensorflow.keras import Sequential, layers, models, Input, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from google.colab import files\n",
        "os.listdir('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3NfcNEH6xCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_new = False\n",
        "\n",
        "images_train_dirname = os.path.join('train')\n",
        "images_validation_dirname = os.path.join('validation')\n",
        "\n",
        "\n",
        "def delete_warning():\n",
        "  for f in os.listdir('.'):\n",
        "    if 'train' in f or 'valid' in f:\n",
        "      for i in range(10, 0, -1):\n",
        "        cancel_msg = 'Go to \"Runtime -> Interrupt execution\" to cancel'\n",
        "        print('Erasing data in {0} seconds. {1}'.format(i, cancel_msg))\n",
        "        time.sleep(1)\n",
        "      return\n",
        "\n",
        "\n",
        "def clear_directory():\n",
        "  delete_warning()\n",
        "  for f in os.listdir('.'):\n",
        "    if 'train' in f or 'valid' in f or 'scene' in f:\n",
        "      print('Deleting', f)\n",
        "      if 'zip' in f:\n",
        "        os.remove(f)\n",
        "      else:\n",
        "        shutil.rmtree(f)\n",
        "\n",
        "\n",
        "def parse_examples(location, overwrite=False):\n",
        "  print(location)\n",
        "  if overwrite and os.path.isdir(location):\n",
        "    print('\\tErasing directory')\n",
        "    shutil.rmtree(location)\n",
        "  if not os.path.isdir(location):\n",
        "    print('\\tUnzipping')\n",
        "    zip_ref = zipfile.ZipFile(location + '.zip', 'r')\n",
        "    zip_ref.extractall(os.path.dirname(location))\n",
        "    zip_ref.close()\n",
        "  img_dir = os.listdir(os.path.join(location, 'face'))\n",
        "  print('\\tFound', len(img_dir), 'examples')\n",
        "  return len(img_dir)\n",
        "\n",
        "\n",
        "if upload_new:\n",
        "  clear_directory()\n",
        "  files.upload()\n",
        "total_train_examples = parse_examples(\n",
        "  images_train_dirname, overwrite=True)\n",
        "total_validation_examples = parse_examples(\n",
        "  images_validation_dirname, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhpRzsjYE5t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 160\n",
        "\n",
        "\n",
        "def create_train_generators(batch_size=1, shuffle=True):\n",
        "  img_options = {\n",
        "    'rescale': 1 / 255\n",
        "  }\n",
        "  img_dir_options = {\n",
        "    'target_size': (img_size, img_size),\n",
        "    'batch_size': batch_size,\n",
        "    'class_mode': 'input',\n",
        "    'shuffle': shuffle,\n",
        "    'classes': ['face']\n",
        "  }\n",
        "  # block print statements\n",
        "  old_stdout = sys.stdout\n",
        "  sys.stdout = open(os.devnull, 'w')\n",
        "  train_datagen = ImageDataGenerator(**img_options).flow_from_directory(\n",
        "    images_train_dirname, **img_dir_options)\n",
        "  validation_datagen = ImageDataGenerator(**img_options).flow_from_directory(\n",
        "    images_validation_dirname, **img_dir_options)\n",
        "  # reenable print statements\n",
        "  sys.stdout = old_stdout\n",
        "  return train_datagen, validation_datagen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xFKh2NFRJoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_encoder_decoder(latent_units=128,\n",
        "                           conv_filters=[32, 64, 128, 256],\n",
        "                           dense_units=[],\n",
        "                           latent_activation=None,\n",
        "                           final_activation=None,\n",
        "                           show_summary=False,\n",
        "                           **kwargs):\n",
        "  '''\n",
        "    ENCODER model\n",
        "  '''\n",
        "  enc_conv_settings = {\n",
        "    'kernel_size': [3, 3],\n",
        "    'padding': 'valid',\n",
        "    'use_bias': True,\n",
        "    'activation': None\n",
        "  }\n",
        "  enc_relu_settings = {\n",
        "    'max_value': None,\n",
        "    'negative_slope': 0.0\n",
        "  }\n",
        "  # INPUT\n",
        "  encoder = Sequential()\n",
        "  # CONV layers\n",
        "  for i, f in enumerate(conv_filters):\n",
        "    if i == 0:\n",
        "      encoder.add(layers.Conv2D(\n",
        "        f, input_shape=[img_size, img_size, 3], **enc_conv_settings))\n",
        "    else:\n",
        "      encoder.add(layers.Conv2D(f, **enc_conv_settings))\n",
        "    encoder.add(layers.BatchNormalization())\n",
        "    encoder.add(layers.ReLU(**enc_relu_settings))\n",
        "    encoder.add(layers.MaxPool2D())\n",
        "  # hidden DENSE layers\n",
        "  encoder.add(layers.Flatten())\n",
        "  flattened_units = encoder.output_shape[-1]  # save flattened shape for decoder\n",
        "  for u in dense_units:\n",
        "    encoder.add(layers.Dense(u, activation='relu'))\n",
        "  # LATENT space\n",
        "  encoder.add(layers.Dense(latent_units, activation=latent_activation))\n",
        "  if show_summary:\n",
        "    encoder.summary()\n",
        "  '''\n",
        "    DECODER model\n",
        "  '''\n",
        "  dec_dense_units = dense_units[::-1]\n",
        "  dec_conv_filters = conv_filters[::-1]\n",
        "  hidden_conv_dim = int(img_size / (2 ** len(dec_conv_filters)))\n",
        "  pre_conv_depth = int(flattened_units / (hidden_conv_dim * hidden_conv_dim))\n",
        "  dec_conv_reshape = [hidden_conv_dim, hidden_conv_dim, pre_conv_depth]\n",
        "  dec_hidden_units = dec_conv_reshape[0] * dec_conv_reshape[1] * dec_conv_reshape[2]\n",
        "  dec_conv_settings = enc_conv_settings.copy()\n",
        "  dec_conv_settings.update(\n",
        "      {'padding': 'same', 'activation': 'relu', 'strides': [2, 2]})\n",
        "  final_conv_settings = dec_conv_settings.copy()\n",
        "  final_conv_settings.update({'activation': final_activation, 'strides': [1, 1]})\n",
        "  # INPUT\n",
        "  decoder = Sequential()\n",
        "  # hidden DENSE layers\n",
        "  for i, u in enumerate(dec_dense_units):\n",
        "    if i == 0:\n",
        "      decoder.add(layers.Dense(\n",
        "        u, input_shape=[latent_units], activation='relu'))\n",
        "    else:\n",
        "      decoder.add(layers.Dense(u, activation='relu'))\n",
        "  # CONV layers\n",
        "  if len(dec_dense_units) == 0:\n",
        "    decoder.add(layers.Dense(\n",
        "      dec_hidden_units, input_shape=[latent_units], activation='relu'))\n",
        "  else:\n",
        "    decoder.add(layers.Dense(dec_hidden_units, activation='relu'))\n",
        "  decoder.add(layers.Reshape(dec_conv_reshape))\n",
        "  for f in dec_conv_filters:\n",
        "    decoder.add(layers.Conv2DTranspose(f, **dec_conv_settings))\n",
        "  # final CONV layer\n",
        "  decoder.add(layers.Conv2DTranspose(3, **final_conv_settings))\n",
        "  if show_summary:\n",
        "    decoder.summary()\n",
        "  return encoder, decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W2lb8fCcf4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create global variable for saving histories\n",
        "# but only if it has not been defined yet\n",
        "try:\n",
        "  saved_histories\n",
        "except NameError:\n",
        "  for i in range(3, 0, -1):\n",
        "    print('Creating new saved_histories in {0} seconds'.format(i))\n",
        "    time.sleep(1)\n",
        "  saved_histories = []\n",
        "\n",
        "\n",
        "def plot_history(hist, ignore=0):\n",
        "  if hasattr(hist, 'history'):\n",
        "    hist = hist.history\n",
        "  epochs = list(range(len(hist['loss'])))\n",
        "  ignore = max(min(ignore, len(hist['loss']) - ignore), 0)\n",
        "  for k in hist.keys():\n",
        "    if 'val_' in k:\n",
        "      continue\n",
        "    if 'lr' in k:\n",
        "      plt.plot(hist[k][ignore:], hist['loss'][ignore:], label='lr & Loss')\n",
        "    else:\n",
        "      plt.plot(\n",
        "        epochs[ignore:], hist[k][ignore:], label='Train {0}'.format(k))\n",
        "      plt.plot(\n",
        "        epochs[ignore:], hist['val_' + k][ignore:], label='Valid {0}'.format(k))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_saved_histories(hists, key, p_label=None, ignore=0):\n",
        "  if len(hists) == 0:\n",
        "    return\n",
        "  for i, hist in enumerate(hists):\n",
        "    h = hist['history']\n",
        "    if hasattr(h, 'history'):\n",
        "      h = h.history\n",
        "    label = '[{0}]'.format(i)\n",
        "    if p_label:\n",
        "      l = hist['params'][p_label]\n",
        "      if l is None:\n",
        "        l = 'None'\n",
        "      label = '{0} {1}'.format(label, l)\n",
        "    epochs = range(ignore, len(h[key]))\n",
        "    plt.plot(epochs, h[key][ignore:], label=label)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  for i, hist in enumerate(hists):\n",
        "    h = hist['history']\n",
        "    if hasattr(h, 'history'):\n",
        "      h = h.history\n",
        "    lbl = i\n",
        "    if p_label:\n",
        "      lbl = hist['params'][p_label]\n",
        "    print(lbl, '--> {:.5E}'.format(min(h[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCLEN6_uTsWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DisplayAutoencoderCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self, model, generator, test_step=1):\n",
        "    tf.keras.callbacks.Callback.__init__(self)\n",
        "    self.model = model\n",
        "    self.generator = generator\n",
        "    self.test_step = test_step\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    if epoch == 0:\n",
        "      draw_random_prediction(self.model, gen=self.generator)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch % self.test_step == 0:\n",
        "      draw_random_prediction(self.model, gen=self.generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLMsVJbnxOba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_and_test_model(graph_label=None,\n",
        "                         epochs=100,\n",
        "                         batch_size=128,\n",
        "                         show_summary=False,\n",
        "                         **kwargs):\n",
        "  print('####################')\n",
        "  print('Appending to {0} Saved Histories'.format(len(saved_histories)))\n",
        "  model_params = {\n",
        "    'latent_units': kwargs.get('latent_units', 128),\n",
        "    'conv_filters': kwargs.get('conv_filters', [32, 64, 128, 256]),\n",
        "    'dense_units': kwargs.get('dense_units', []),\n",
        "    'latent_activation': kwargs.get('latent_activation', None),\n",
        "    'final_activation': kwargs.get('final_activation', None),\n",
        "    'learning_rate': kwargs.get('learning_rate', 0.001),\n",
        "    'epsilon': kwargs.get('epsilon', 1e-7),\n",
        "    'batch_size': batch_size\n",
        "  }\n",
        "  for key, val in model_params.items():\n",
        "    arrow = ''\n",
        "    if graph_label and key == graph_label:\n",
        "      arrow = '<======='\n",
        "    print('  -', key, ':', val, arrow)\n",
        "  if graph_label:\n",
        "    assert graph_label in model_params\n",
        "  ''' Build the Model(s) '''\n",
        "  encoder, decoder = create_encoder_decoder(\n",
        "    show_summary=show_summary, **model_params)\n",
        "  if show_summary:\n",
        "    return\n",
        "  model_autoencoder = Sequential([encoder, decoder])\n",
        "  ''' Train/Validation Generator '''\n",
        "  train_gen, val_gen = create_train_generators(batch_size, shuffle=True)\n",
        "  ''' Callbacks during training '''\n",
        "  progress = tf.keras.utils.Progbar(epochs, unit_name='Epochs')\n",
        "  callbacks = [\n",
        "  #   DisplayAutoencoderCallback(model_autoencoder, val_gen, test_step=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "  ]\n",
        "  if graph_label:\n",
        "    callbacks.append(tf.keras.callbacks.LambdaCallback(\n",
        "      on_epoch_begin=lambda epoch, logs: progress.update(epoch),\n",
        "      on_train_end=lambda logs: progress.update(epochs)\n",
        "    ))\n",
        "  ''' Compile '''\n",
        "  model_autoencoder.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "      learning_rate=model_params['learning_rate'],\n",
        "      epsilon=model_params['epsilon']\n",
        "    ),\n",
        "    loss='mse'\n",
        "  )\n",
        "  ''' Train '''\n",
        "  print('Will exit training once \"val_loss\" no longer decreases')\n",
        "  verbose = 1\n",
        "  if graph_label:\n",
        "    verbose = 0\n",
        "  history = model_autoencoder.fit_generator(\n",
        "    train_gen,\n",
        "    steps_per_epoch=int(total_train_examples / batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=int(total_train_examples / batch_size),\n",
        "    verbose=verbose,\n",
        "    callbacks=callbacks\n",
        "  )\n",
        "  print()\n",
        "  ''' Save History and Params '''\n",
        "  saved_histories.append({\n",
        "    'history': history.history,\n",
        "    'params': model_params,\n",
        "    'model': model_autoencoder,\n",
        "    'encoder': encoder,\n",
        "    'decoder': decoder\n",
        "  })\n",
        "  if graph_label:\n",
        "    plot_saved_histories(\n",
        "      saved_histories, 'val_loss', p_label=graph_label, ignore=10)\n",
        "  else:\n",
        "    plot_history(saved_histories[-1]['history'], ignore=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amHaUDv8yzSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "build_and_test_model(\n",
        "  batch_size=32,\n",
        "  learning_rate=0.00001,\n",
        "  epochs=1000\n",
        ")\n",
        "\n",
        "# for b in [16, 32, 64, 128, 256]:\n",
        "#   build_and_test_model(\n",
        "#     graph_label='batch_size',\n",
        "#     batch_size=32,\n",
        "#     learning_rate=0.0001,\n",
        "#     epochs=1000\n",
        "#   )\n",
        "\n",
        "# for lr in [0.005, 0.001, 0.0001, 0.00001]:\n",
        "#   build_and_test_model(\n",
        "#     graph_label='learning_rate',\n",
        "#     batch_size=32,\n",
        "#     learning_rate=lr,\n",
        "#     epochs=1000\n",
        "#   )\n",
        "\n",
        "# for units in [8, 16, 32, 64, 128, 256]:\n",
        "#   build_and_test_model(\n",
        "#     graph_label='latent_units',\n",
        "#     latent_units=units,\n",
        "#     batch_size=32,\n",
        "#     learning_rate=0.001,\n",
        "#     epochs=1000\n",
        "#   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnNncBlCjpVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_model_folder = os.path.join('models')\n",
        "model_file_name = 'simpsons_model_{0}'\n",
        "\n",
        "if not os.path.exists(drive_model_folder):\n",
        "    os.makedirs(drive_model_folder)\n",
        "\n",
        "def dated_filename():\n",
        "  return str(datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
        "\n",
        "\n",
        "def save_model(model, name):\n",
        "  blob_name = '{0}_{1}.h5'.format(name, dated_filename())\n",
        "  print('Saving -> {}'.format(blob_name))\n",
        "  file_name = os.path.join(drive_model_folder, blob_name)\n",
        "  model.save(file_name)\n",
        "  return file_name\n",
        "\n",
        "\n",
        "def load_latest_model(name, custom_objects={}):\n",
        "  model_files = [\n",
        "      f\n",
        "      for f in os.listdir(drive_model_folder)\n",
        "      if name in f and 'h5' in f\n",
        "  ]\n",
        "  model_files.sort()\n",
        "  latest_file_name = model_files[-1]\n",
        "  latest_filepath = os.path.join(drive_model_folder, latest_file_name)\n",
        "  print('Loading -> {}'.format(latest_filepath))\n",
        "  model = tf.keras.models.load_model(\n",
        "      latest_filepath, custom_objects=custom_objects)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ZhZXjJHqco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for hist in saved_histories:\n",
        "  b_size = hist['params']['batch_size']\n",
        "  enc_path = save_model(hist['encoder'], 'simpsons_ae_encoder_{0}'.format(b_size))\n",
        "  dec_path = save_model(hist['decoder'], 'simpsons_ae_decoder_{0}'.format(b_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjkXuRwnCH2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_random_prediction(model, count=1, gen=None):\n",
        "  all_images = []\n",
        "  if gen is None:\n",
        "    _, gen = create_train_generators()\n",
        "  for i in range(count):\n",
        "    x, _ = next(gen)\n",
        "    all_images.append(x[0])\n",
        "    guess = model.predict(x)\n",
        "    guess_img = np.clip(guess[0], 0, 1)  # constrain to be 0-1\n",
        "    all_images.append(guess_img)\n",
        "  show_images(all_images, cols=count)\n",
        "\n",
        "\n",
        "def show_images(images, cols=1):\n",
        "    n_images = len(images)\n",
        "    fig = plt.figure()\n",
        "    for n, image in enumerate(images):\n",
        "      index = int(n / 2)\n",
        "      if n % 2 == 1:\n",
        "        index += int(n_images / 2)\n",
        "      a = fig.add_subplot(2, cols, index + 1)\n",
        "      plt.imshow(image)\n",
        "    size = 3\n",
        "    fig.set_size_inches(int(size * cols * 0.5), size)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue60HlBj_QnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for hist in saved_histories:\n",
        "  print(hist['params'])\n",
        "  draw_random_prediction(hist['model'], count=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKFf7yTSs8hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(os.path.getsize(enc_path))\n",
        "files.download(enc_path)\n",
        "print(os.path.getsize(dec_path))\n",
        "files.download(dec_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qiF8fcUB-LP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "upload_model_h5 = False\n",
        "\n",
        "if upload_model_h5:\n",
        "  files.upload()\n",
        "  for f in os.listdir():\n",
        "    if '.h5' in f:\n",
        "      shutil.move(f, os.path.join(drive_model_folder, f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q4-s0-NScUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_prebuilt_model = False\n",
        "\n",
        "if load_prebuilt_model:\n",
        "  encoder = load_latest_model('simpsons_ae_encoder')\n",
        "  decoder = load_latest_model('simpsons_ae_decoder')\n",
        "  model_autoencoder = Sequential([encoder, decoder])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuWz8kIJaAg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_histories = []\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyA-foXKaC-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
