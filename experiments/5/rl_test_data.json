[{"returns": [62.400001525878906, 36.599998474121094, 110.5999984741211, 75.80000305175781, 118.19999694824219, 108.5999984741211, 155.8000030517578, 219.39999389648438, 291.3999938964844, 176.39999389648438, 288.0, 232.60000610351562, 99.80000305175781, 114.0, 211.39999389648438, 220.8000030517578, 218.8000030517578, 140.0, 128.0], "params": {"env_name": "CartPole-v1", "agent": "reinforce", "on_policy": true, "learning_rate": 0.001, "has_rnn_networks": false, "lstm_size": null, "normalize_returns": true, "has_value_network": true, "normalize_rewards": false, "normalize_observations": false, "td_errors_loss_fn": "element_wise_squared_loss", "actor_fc_layer_params": [100], "actor_conv_layer_params": null, "rnn_actor_output_fc_layer_params": null, "value_fc_layer_params": [100], "value_conv_layer_params": null, "rnn_value_output_fc_layer_params": null, "qnet_fc_layer_params": [100], "qnet_rnn_output_fc_layer_params": [100], "replay_buffer_max_length": 150000, "off_policy_train_batch_size": 64, "on_policy_collect_episodes": 20, "off_policy_collect_steps": 1, "eval_episodes": 5, "num_train_loops": 100, "off_policy_train_steps": 10000, "off_policy_pre_collect": 1000, "off_policy_train_iterations_per_loop": 1, "max_seq_reward_drops": 10, "ddpg_ou_stddev": 1.0, "ddpg_ou_damping": 1.0, "critic_observation_fc_layer_params": [100], "critic_observation_conv_layer_params": null, "critic_action_fc_layer_params": [100], "critic_joint_fc_layer_params": [100], "rnn_critic_output_fc_layer_params": [100]}}, {"returns": [37.20000076293945, 55.20000076293945, 84.19999694824219, 111.80000305175781, 155.8000030517578, 152.60000610351562, 161.39999389648438, 165.0, 173.39999389648438, 191.0, 169.8000030517578, 182.8000030517578, 193.60000610351562, 338.3999938964844, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0], "params": {"env_name": "CartPole-v1", "agent": "ppo", "on_policy": true, "learning_rate": 0.001, "has_rnn_networks": false, "lstm_size": null, "normalize_returns": true, "has_value_network": true, "normalize_rewards": false, "normalize_observations": false, "td_errors_loss_fn": "element_wise_squared_loss", "actor_fc_layer_params": [100], "actor_conv_layer_params": null, "rnn_actor_output_fc_layer_params": null, "value_fc_layer_params": [100], "value_conv_layer_params": null, "rnn_value_output_fc_layer_params": null, "qnet_fc_layer_params": [100], "qnet_rnn_output_fc_layer_params": [100], "replay_buffer_max_length": 150000, "off_policy_train_batch_size": 64, "on_policy_collect_episodes": 20, "off_policy_collect_steps": 1, "eval_episodes": 5, "num_train_loops": 100, "off_policy_train_steps": 10000, "off_policy_pre_collect": 1000, "off_policy_train_iterations_per_loop": 1, "max_seq_reward_drops": 10, "ddpg_ou_stddev": 1.0, "ddpg_ou_damping": 1.0, "critic_observation_fc_layer_params": [100], "critic_observation_conv_layer_params": null, "critic_action_fc_layer_params": [100], "critic_joint_fc_layer_params": [100], "rnn_critic_output_fc_layer_params": [100]}}, {"returns": [9.199999809265137, 9.0, 10.0, 9.199999809265137, 9.0, 12.0, 34.20000076293945, 33.0, 100.4000015258789, 83.4000015258789, 176.1999969482422, 87.19999694824219, 102.19999694824219, 41.79999923706055, 49.0, 61.79999923706055, 31.200000762939453, 31.0, 30.200000762939453, 39.0, 27.399999618530273], "params": {"env_name": "CartPole-v1", "agent": "dqn", "on_policy": false, "learning_rate": 0.001, "has_rnn_networks": false, "lstm_size": null, "normalize_returns": true, "has_value_network": true, "normalize_rewards": false, "normalize_observations": false, "td_errors_loss_fn": "element_wise_squared_loss", "actor_fc_layer_params": [100], "actor_conv_layer_params": null, "rnn_actor_output_fc_layer_params": null, "value_fc_layer_params": [100], "value_conv_layer_params": null, "rnn_value_output_fc_layer_params": null, "qnet_fc_layer_params": [100], "qnet_rnn_output_fc_layer_params": [100], "replay_buffer_max_length": 150000, "off_policy_train_batch_size": 64, "on_policy_collect_episodes": 20, "off_policy_collect_steps": 1, "eval_episodes": 5, "num_train_loops": 100, "off_policy_train_steps": 10000, "off_policy_pre_collect": 1000, "off_policy_train_iterations_per_loop": 1, "max_seq_reward_drops": 10, "ddpg_ou_stddev": 1.0, "ddpg_ou_damping": 1.0, "critic_observation_fc_layer_params": [100], "critic_observation_conv_layer_params": null, "critic_action_fc_layer_params": [100], "critic_joint_fc_layer_params": [100], "rnn_critic_output_fc_layer_params": [100]}}, {"returns": [44.599998474121094, 9.199999809265137, 9.399999618530273, 18.799999237060547, 14.199999809265137, 9.800000190734863, 18.799999237060547, 13.199999809265137, 21.799999237060547, 66.5999984741211, 24.600000381469727, 22.399999618530273, 68.80000305175781, 25.0, 27.200000762939453, 22.200000762939453, 18.799999237060547, 25.399999618530273, 23.0, 20.0, 39.0, 32.599998474121094, 22.799999237060547], "params": {"env_name": "CartPole-v1", "agent": "ddqn", "on_policy": false, "learning_rate": 0.001, "has_rnn_networks": false, "lstm_size": null, "normalize_returns": true, "has_value_network": true, "normalize_rewards": false, "normalize_observations": false, "td_errors_loss_fn": "element_wise_squared_loss", "actor_fc_layer_params": [100], "actor_conv_layer_params": null, "rnn_actor_output_fc_layer_params": null, "value_fc_layer_params": [100], "value_conv_layer_params": null, "rnn_value_output_fc_layer_params": null, "qnet_fc_layer_params": [100], "qnet_rnn_output_fc_layer_params": [100], "replay_buffer_max_length": 150000, "off_policy_train_batch_size": 64, "on_policy_collect_episodes": 20, "off_policy_collect_steps": 1, "eval_episodes": 5, "num_train_loops": 100, "off_policy_train_steps": 10000, "off_policy_pre_collect": 1000, "off_policy_train_iterations_per_loop": 1, "max_seq_reward_drops": 10, "ddpg_ou_stddev": 1.0, "ddpg_ou_damping": 1.0, "critic_observation_fc_layer_params": [100], "critic_observation_conv_layer_params": null, "critic_action_fc_layer_params": [100], "critic_joint_fc_layer_params": [100], "rnn_critic_output_fc_layer_params": [100]}}, {"returns": [9.199999809265137, 9.0, 9.600000381469727, 9.0, 8.600000381469727, 9.199999809265137, 9.199999809265137, 9.600000381469727, 9.399999618530273, 9.600000381469727, 10.0, 9.800000190734863, 8.800000190734863, 9.0, 10.199999809265137, 9.0, 8.800000190734863, 9.399999618530273, 9.199999809265137, 10.0, 9.399999618530273, 9.199999809265137, 8.800000190734863, 9.800000190734863, 9.199999809265137], "params": {"env_name": "CartPole-v1", "agent": "ddpg", "on_policy": false, "learning_rate": 0.001, "has_rnn_networks": false, "lstm_size": null, "normalize_returns": true, "has_value_network": true, "normalize_rewards": false, "normalize_observations": false, "td_errors_loss_fn": "element_wise_squared_loss", "ddpg_ou_stddev": 1.0, "ddpg_ou_damping": 1.0, "actor_fc_layer_params": [100], "actor_conv_layer_params": null, "rnn_actor_output_fc_layer_params": null, "value_fc_layer_params": [100], "value_conv_layer_params": null, "rnn_value_output_fc_layer_params": null, "qnet_fc_layer_params": [100], "qnet_rnn_output_fc_layer_params": [100], "critic_observation_fc_layer_params": [100], "critic_observation_conv_layer_params": null, "critic_action_fc_layer_params": [100], "critic_joint_fc_layer_params": [100], "rnn_critic_output_fc_layer_params": [100], "replay_buffer_max_length": 150000, "off_policy_train_batch_size": 64, "on_policy_collect_episodes": 20, "off_policy_collect_steps": 1, "eval_episodes": 5, "num_train_loops": 100, "off_policy_train_steps": 10000, "off_policy_pre_collect": 1000, "off_policy_train_iterations_per_loop": 1, "max_seq_reward_drops": 10, "continuous_action": true}}]